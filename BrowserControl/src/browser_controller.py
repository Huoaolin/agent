from playwright.async_api import async_playwright
from .task_manager import TaskManager
from .action_result import ActionResult
from bs4 import BeautifulSoup
import re
import asyncio
from .llm_call import LLMCall
import json


class BrowserController:
    def __init__(self):
        self.playwright = None
        self.browser = None
        self.context = None
        self.page = None
        self.observers = []
        self.llm = LLMCall()

    async def start(self):
        self.playwright = await async_playwright().start()
        # ä¼˜åŒ–æµè§ˆå™¨å¯åŠ¨ï¼šå…³é—­ headless æ¨¡å¼ï¼ˆå¯é€‰ï¼‰ï¼Œæ·»åŠ æ›´å¤šåçˆ¬é…ç½®
        self.browser = await self.playwright.chromium.launch(
            headless=True,  # å¯æ”¹ä¸º False ç”¨äºè°ƒè¯•
            args=['--no-sandbox', '--disable-blink-features=AutomationControlled']
        )
        self.context = await self.browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            viewport={"width": 1280, "height": 720},  # è®¾ç½®çª—å£å¤§å°
            java_script_enabled=True,
            ignore_https_errors=True  # å¿½ç•¥ HTTPS é”™è¯¯
        )
        self.page = await self.context.new_page()
        # æ·»åŠ åçˆ¬è„šæœ¬ï¼Œä¼ªè£… webdriver å’Œå…¶ä»–ç‰¹æ€§
        await self.page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
            Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3] });
            Object.defineProperty(navigator, 'languages', { get: () => ['zh-CN', 'zh'] });
        """)
        print("æµè§ˆå™¨åˆå§‹åŒ–æˆåŠŸ")
        self.notify_observers("Browser initialized")

    async def open_page(self, url):
        """æ‰“å¼€æŒ‡å®š URLï¼Œä¼˜åŒ–åŠ è½½ç­‰å¾…å’Œåçˆ¬å¤„ç†"""
        try:
            # è®¾ç½®ä»£ç†ï¼ˆå¦‚æœéœ€è¦ï¼Œéœ€æ›¿æ¢ä¸ºå®é™…ä»£ç†åœ°å€ï¼‰
            # await self.page.setExtraHTTPHeaders({"Proxy-Server": "http://your_proxy:port"})
            # å¢åŠ è¶…æ—¶æ—¶é—´å¹¶æ”¾å®½ç­‰å¾…æ¡ä»¶
            print(f"å¼€å§‹å¯¼èˆªåˆ°: {url}")
            await self.page.goto(url, wait_until="domcontentloaded", timeout=60000)  # ä» networkidle æ”¹ä¸º domcontentloaded
            await self.page.wait_for_load_state("domcontentloaded")  # ç¡®ä¿ DOM åŠ è½½å®Œæˆ
            # æ£€æŸ¥æ˜¯å¦é‡åˆ°éªŒè¯ç æˆ–ç™»å½•é¡µé¢
            content = await self.page.content()
            if "éªŒè¯ç " in content or "login.taobao.com" in self.page.url:
                print("æ£€æµ‹åˆ°éªŒè¯ç æˆ–ç™»å½•é¡µé¢ï¼Œå°è¯•ç­‰å¾…æ‰‹åŠ¨å¤„ç†æˆ–è°ƒæ•´ç­–ç•¥")
                await asyncio.sleep(5)  # çŸ­æš‚ç­‰å¾…ï¼Œè§‚å¯Ÿæ˜¯å¦å¯åŠ è½½
                content = await self.page.content()  # é‡æ–°è·å–å†…å®¹
            current_url = self.page.url
            title = await self.page.title()
            msg = f"ğŸ”—  Navigated to {url} (Loaded URL: {current_url}, Title: {title})"
            self.notify_observers(msg)
            return content
        except Exception as e:
            error_msg = f"æ‰§è¡Œä»»åŠ¡ open_page æ—¶å‡ºé”™: {e}"
            self.notify_observers(error_msg)
            print(f"é”™è¯¯è¯¦æƒ…: {e}")
            print(f"å½“å‰é¡µé¢å†…å®¹: {content[:200] if 'content' in locals() else 'æœªåŠ è½½'}")
            raise Exception(error_msg)

    async def search(self, query):
        """åœ¨å½“å‰é¡µé¢æ‰§è¡Œæœç´¢ï¼Œæ”¯æŒç™¾åº¦æˆ–æ·˜å®"""
        try:
            current_url = self.page.url if self.page.url else "https://www.taobao.com"
            if "baidu.com" in current_url:
                await self.page.goto("https://www.baidu.com", wait_until="domcontentloaded", timeout=60000)
                await self.page.fill("input#kw", query)
                await self.page.press("input#kw", "Enter")
                await self.page.wait_for_load_state("domcontentloaded")
                await asyncio.sleep(2)  # ç­‰å¾…åŠ¨æ€å†…å®¹
                msg = f"ğŸ”  Searched for '{query}' on Baidu"
            elif "taobao.com" in current_url:
                await self.page.fill("input#q", query)  # æ·˜å®æœç´¢æ¡†
                await self.page.press("input#q", "Enter")
                await self.page.wait_for_load_state("domcontentloaded")
                await asyncio.sleep(2)
                msg = f"ğŸ”  Searched for '{query}' on Taobao"
            else:
                await self.page.goto(f"https://www.google.com/search?q={query}&udm=14", wait_until="domcontentloaded", timeout=60000)
                msg = f"ğŸ”  Searched for '{query}' on Google"
            self.notify_observers(msg)
            return await self.page.content()
        except Exception as e:
            error_msg = f"Search failed for '{query}': {str(e)}"
            self.notify_observers(error_msg)
            raise Exception(error_msg)

    async def click_element(self, selector):
        """ç‚¹å‡»æŒ‡å®šå…ƒç´ ï¼Œæ·»åŠ å¯è§æ€§æ£€æŸ¥"""
        try:
            await self.page.wait_for_selector(selector, state="visible", timeout=30000)
            await self.page.locator(selector).click()
            msg = f"ğŸ–±ï¸  Clicked element: {selector}"
            self.notify_observers(msg)
            await self.page.wait_for_load_state("domcontentloaded")
            return await self.page.content()
        except Exception as e:
            error_msg = f"Click element failed for '{selector}': {str(e)}"
            self.notify_observers(error_msg)
            raise Exception(error_msg)

    async def extract_text(self, query):
        """ä½¿ç”¨è§„åˆ™å‡½æ•°é¢„å¤„ç† HTMLï¼Œå†ç”± LLM æå–ä¸ query ç›¸å…³çš„ä¿¡æ¯"""
        try:
            html_content = await self.page.content()
            if not html_content:
                error_msg = "Page content is empty"
                self.notify_observers(error_msg)
                return error_msg

            structured_text = await self._extract_structured_text(html_content)
            self.notify_observers(f"ğŸ“‹  Structured text extracted: {structured_text[:200]}...")

            prompt = f"""
            ä»ä»¥ä¸‹ç»“æ„åŒ–ç½‘é¡µå†…å®¹ä¸­æå–ä¸ '{query}' ç›¸å…³çš„ä¿¡æ¯ã€‚
            - æŸ¥è¯¢å†…å®¹å¯èƒ½æ˜¯ä»»ä½•ç±»å‹çš„ä¿¡æ¯ï¼Œè¯·æ ¹æ®ä¸Šä¸‹æ–‡å°½å¯èƒ½æå–ç›¸å…³å†…å®¹ã€‚
            - å¦‚æœæŸ¥è¯¢åŒ…å«å…·ä½“å¯¹è±¡ï¼Œå³ä½¿å†…å®¹ä¸­æœªæ˜ç¡®æåŠè¯¥å¯¹è±¡ï¼Œåªè¦ä¿¡æ¯åœ¨ä¸Šä¸‹æ–‡ä¸­åˆç†ç›¸å…³ï¼Œä¹Ÿåº”æå–ã€‚
            - è¿”å›æå–åˆ°çš„ä¿¡æ¯ï¼Œå¯ä»¥æ˜¯ç®€æ´çš„æ–‡æœ¬æˆ–æŒ‰ç±»åˆ«ç»„ç»‡çš„é”®å€¼å¯¹ï¼Œå…·ä½“æ ¼å¼æ ¹æ®å†…å®¹è‡ªç„¶é€‰æ‹©ã€‚
            è¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - "result": æå–åˆ°çš„ä¿¡æ¯ï¼ˆå­—ç¬¦ä¸²æˆ–å­—å…¸ï¼‰
            - "status": "success" æˆ– "error"
            å¦‚æœæ‰¾ä¸åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¿”å› "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"ã€‚
            ç»“æ„åŒ–å†…å®¹ï¼š
            {structured_text}
            """
            if asyncio.iscoroutinefunction(self.llm.call):
                raw_response = await self.llm.call(prompt, response_format={"type": "json_object"})
            else:
                raw_response = self.llm.call(prompt, response_format={"type": "json_object"})

            self.notify_observers(f"ğŸ“  LLM raw response: {raw_response}")
            if isinstance(raw_response, str):
                response = json.loads(raw_response)
            else:
                response = raw_response

            if response.get("status") == "success":
                # å°† query å’Œæå–ç»“æœåŒ…è£…ä¸ºå­—å…¸
                extracted_result = response.get("result", "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯")
                result = {
                    "query": query,
                    "extracted": extracted_result
                }
            else:
                result = "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"  # å¤±è´¥æ—¶ä¿æŒåŸé€»è¾‘

            self.notify_observers(f"ğŸ“  LLM extracted: {result}")
            return result
        except Exception as e:
            error_msg = f"Failed to extract text with LLM for '{query}': {str(e)}"
            self.notify_observers(error_msg)
            return error_msg

    async def extract_image(self, query):
        n = 3
        """ä»é¡µé¢ä¸­æå–ä¸ query æœ€ç›¸å…³çš„ N ä¸ªå›¾ç‰‡ URL å’Œæ ‡é¢˜å¯¹"""
        try:
            # è·å–å½“å‰é¡µé¢å®Œæ•´å†…å®¹
            html_content = await self.page.content()
            if not html_content:
                error_msg = "Page content is empty"
                self.notify_observers(error_msg)
                return error_msg

            # ä½¿ç”¨ BeautifulSoup è§£æ HTML
            soup = BeautifulSoup(html_content, 'html.parser')
            image_title_pairs = []
            # æŸ¥æ‰¾æ‰€æœ‰å¸¦æœ‰ class="cos-image-body" çš„å›¾ç‰‡
            for img in soup.find_all('img', class_='cos-image-body'):
                src = img.get('src')
                alt = img.get('alt', '')
                if src and src.startswith('http'):  # åªä¿ç•™æœ‰æ•ˆçš„å›¾ç‰‡ URL
                    # æŸ¥æ‰¾æœ€è¿‘çš„æ ‡é¢˜ï¼ˆclass="cosc-title-slot"ï¼‰
                    title_tag = img.find_next('span', class_='cosc-title-slot')
                    title = ""
                    if title_tag:
                        inner_span = title_tag.find('span')
                        title = inner_span.get_text(strip=True) if inner_span else title_tag.get_text(strip=True)
                    image_title_pairs.append({
                        "url": src,
                        "alt": alt,
                        "title": title if title else "æ— æ ‡é¢˜",
                        "context": self._get_image_context(img, soup)
                    })
            if not image_title_pairs:
                error_msg = "No image-title pairs found on the page"
                self.notify_observers(error_msg)
                return error_msg
            # å°†å›¾ç‰‡-æ ‡é¢˜å¯¹ä¿¡æ¯ä¼ é€’ç»™ LLM
            pairs_json = json.dumps(image_title_pairs, ensure_ascii=False)
            self.notify_observers(f"ğŸ–¼ï¸  Image-title pairs extracted: {pairs_json[:200]}...")
            self.notify_observers(f"ğŸ–¼ï¸ *************DEBUG*************")
            self.notify_observers(f"ğŸ–¼ï¸ {pairs_json}")
            self.notify_observers(f"ğŸ–¼ï¸ *************DEBUG*************")
            # ä¿®æ”¹ promptï¼Œè¦æ±‚è¿”å› N ä¸ªç›¸å…³å›¾ç‰‡
            prompt = f"""
            ä»ä»¥ä¸‹é¡µé¢ä¸­æå–çš„å›¾ç‰‡-æ ‡é¢˜å¯¹ä¿¡æ¯ä¸­ï¼Œæ‰¾å‡ºä¸ '{query}' æœ€ç›¸å…³çš„ {n} å¼ å›¾ç‰‡ã€‚
            - æŸ¥è¯¢å†…å®¹å¯èƒ½æ˜¯ä»»ä½•ç±»å‹çš„ä¿¡æ¯ï¼Œè¯·æ ¹æ®å›¾ç‰‡çš„ URLã€alt å±æ€§ã€æ ‡é¢˜æˆ–å‘¨å›´ä¸Šä¸‹æ–‡ï¼Œåˆ¤æ–­å“ªäº›å›¾ç‰‡æœ€ç¬¦åˆæŸ¥è¯¢æ„å›¾ã€‚
            - å¦‚æœæŸ¥è¯¢åŒ…å«å…·ä½“å¯¹è±¡ï¼Œå³ä½¿ä¿¡æ¯ä¸­æœªæ˜ç¡®æåŠè¯¥å¯¹è±¡ï¼Œåªè¦æ ‡é¢˜æˆ–ä¸Šä¸‹æ–‡åˆç†ç›¸å…³ï¼Œä¹Ÿåº”é€‰æ‹©ç›¸å…³å›¾ç‰‡ã€‚
            - è¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
            - "results": ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
              - "url": å›¾ç‰‡çš„ URLï¼ˆå­—ç¬¦ä¸²ï¼‰
              - "title": å›¾ç‰‡çš„æ ‡é¢˜ï¼ˆå­—ç¬¦ä¸²ï¼‰
            - "status": "success" æˆ– "error"
            - å¦‚æœç›¸å…³å›¾ç‰‡å°‘äº {n} å¼ ï¼Œè¿”å›æ‰€æœ‰ç›¸å…³å›¾ç‰‡ï¼›å¦‚æœæ²¡æœ‰ç›¸å…³å›¾ç‰‡ï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚
            å›¾ç‰‡-æ ‡é¢˜å¯¹ä¿¡æ¯ï¼š
            {pairs_json}
            """
            if asyncio.iscoroutinefunction(self.llm.call):
                raw_response = await self.llm.call(prompt, response_format={"type": "json_object"})
            else:
                raw_response = self.llm.call(prompt, response_format={"type": "json_object"})

            self.notify_observers(f"ğŸ“  LLM raw response: {raw_response}")
            if isinstance(raw_response, str):
                response = json.loads(raw_response)
            else:
                response = raw_response

            if response.get("status") == "success":
                # æå– LLM è¿”å›çš„å›¾ç‰‡åˆ—è¡¨
                results_list = response.get("results", [])
                # è½¬æ¢ä¸ºæ‰€éœ€çš„æ ¼å¼
                result = {
                    "query": query,
                    "images": [
                        {"extracted": item["url"], "title": item["title"]}
                        for item in results_list
                    ]
                }
            else:
                result = {
                    "query": query,
                    "images": []  # å¦‚æœå¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
                }
            self.notify_observers(f"ğŸ–¼ï¸  LLM extracted image-title pairs: {result}")
            return result
        except Exception as e:
            error_msg = f"Failed to extract images with LLM for '{query}': {str(e)}"
            self.notify_observers(error_msg)
            return error_msg

    def _get_image_context(self, img_tag, soup):
        """ä»æ›´å®½æ³›çš„èŒƒå›´å†…æå–å›¾ç‰‡çš„ä¸Šä¸‹æ–‡æ–‡æœ¬ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰"""
        context = ""
        alt_text = img_tag.get('alt', '').strip()
        if alt_text:
            context += alt_text + " "
        parent = img_tag.parent
        if parent:
            parent_text = parent.get_text(strip=True)
            if parent_text and len(parent_text) > len(context):
                context = parent_text
            for sibling in parent.find_previous_siblings()[:2]:
                sibling_text = sibling.get_text(strip=True)
                if sibling_text:
                    context += " " + sibling_text
            for sibling in parent.find_next_siblings()[:2]:
                sibling_text = sibling.get_text(strip=True)
                if sibling_text:
                    context += " " + sibling_text
        if len(context) < 20 and parent:
            grandparent = parent.parent
            if grandparent:
                grandparent_text = grandparent.get_text(strip=True)
                if grandparent_text:
                    context = grandparent_text
        for ancestor in img_tag.find_parents():
            if ancestor.name in ['h1', 'h2', 'h3', 'p', 'div', 'article']:
                ancestor_text = ancestor.get_text(strip=True)
                if ancestor_text and len(ancestor_text) > len(context):
                    context = ancestor_text
                break
        return context.strip() if context else "æ— ä¸Šä¸‹æ–‡ä¿¡æ¯"

    async def find_most_relevant_link(self, query, previous_state=None):
        """æ‰¾åˆ°é¡µé¢ä¸­æœ€ç›¸å…³çš„é“¾æ¥ URLï¼ŒåŸºäº previous_state æˆ–å½“å‰é¡µé¢"""
        try:
            html_content = previous_state if previous_state else await self.page.content()
            soup = BeautifulSoup(html_content, 'html.parser')
            links = []
            for a_tag in soup.find_all('a', href=True):
                link_text = a_tag.get_text(strip=True)
                href = a_tag['href']
                if link_text and href and href.startswith('http'):
                    links.append({"text": link_text, "url": href})
            if not links:
                self.notify_observers("ğŸ”—  No valid links found")
                return None

            prompt = f"""
            Given the query: '{query}',
            and the following list of links with their text:
            {json.dumps(links, ensure_ascii=False, indent=2)},
            determine which link is most relevant to the query.
            Return a JSON object with:
            - "url": the most relevant URL (or null if none is relevant)
            - "reason": a brief explanation of the choice
            """
            if asyncio.iscoroutinefunction(self.llm.call):
                raw_response = await self.llm.call(prompt, response_format={"type": "json_object"})
            else:
                raw_response = self.llm.call(prompt, response_format={"type": "json_object"})

            self.notify_observer(f"ğŸ”—  LLM raw response for link selection: {raw_response}")
            if isinstance(raw_response, str):
                response = json.loads(raw_response)
            else:
                response = raw_response

            relevant_url = response.get("url")
            reason = response.get("reason", "No reason provided")
            if relevant_url:
                self.notify_observers(f"ğŸ”—  Most relevant link found: {relevant_url} ({reason})")
                return relevant_url
            else:
                self.notify_observers(f"ğŸ”—  No relevant link found: {reason}")
                return None
        except Exception as e:
            error_msg = f"Failed to find relevant link for '{query}': {str(e)}"
            self.notify_observers(error_msg)
            return None

    async def _extract_structured_text(self, html_content):
        """ä» HTML ä¸­æå–è‡ªç„¶è¯­è¨€æ–‡å­—ä¿¡æ¯ï¼Œä¿ç•™é…ç½®ç›¸å…³å†…å®¹"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            for tag in soup(['script', 'style', 'noscript', 'footer', 'nav', 'meta', 'link', 'head']):
                tag.decompose()

            extracted_text = []
            content_containers = soup.find_all(['div', 'section', 'article', 'ul', 'li', 'p', 'span', 'td'])

            text_pattern = re.compile(r'^[\w\s.,;:!?Â¥%-Â®â„¢/\(\)=+]+$', re.UNICODE)
            code_pattern = re.compile(r'[<{][^>]+>|http[s]?://|\b(var|function|json|script)\b', re.I)

            for container in content_containers:
                text = container.get_text(strip=True)
                if (text and len(text) > 2 and
                    text_pattern.match(text) and
                    not code_pattern.search(text) and
                    text not in extracted_text):
                    extracted_text.append(text)

            if extracted_text:
                structured_text = "\n".join(extracted_text)
            else:
                structured_text = "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
            return structured_text
        except Exception as e:
            self.notify_observers(f"Failed to extract structured text: {str(e)}")
            return f"æå–ç»“æ„åŒ–æ–‡æœ¬å¤±è´¥: {str(e)}"

    async def take_screenshot(self, filename):
        """æˆªå–å±å¹•æˆªå›¾"""
        path = f"{TaskManager().get_task_dir()}/{filename}"
        await self.page.screenshot(path=path)
        msg = f"ğŸ“¸  Screenshot saved to: {path}"
        self.notify_observers(msg)
        return path

    async def download_file(self, url, filename):
        """ä¸‹è½½æ–‡ä»¶"""
        async with self.page.expect_download() as download_info:
            await self.page.goto(url)
        download = await download_info.value
        path = f"{TaskManager().get_task_dir()}/{filename}"
        await download.save_as(path)
        self.notify_observers(f"ğŸ’¾  Downloaded file to: {path}")
        return path

    async def close(self):
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
        self.notify_observers("Browser closed")

    def add_observer(self, observer):
        self.observers.append(observer)

    def notify_observers(self, event):
        for observer in self.observers:
            observer.update(event)
